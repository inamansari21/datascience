{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inamansari21/datascience/blob/main/Neural__Network_assng_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_Fa0gHklTjH"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split, cross_val_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kOMQLovWlTjJ"
      },
      "outputs": [],
      "source": [
        "# Loading dataset\n",
        "\n",
        "data = pd.read_csv('forestfires.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wep_462lTjK"
      },
      "outputs": [],
      "source": [
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXs_Nzg3lTjM"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJ8WLmQSlTjN"
      },
      "outputs": [],
      "source": [
        "data.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSzgjrmFlTjO"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJJLO2HjlTjP"
      },
      "outputs": [],
      "source": [
        "data.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Hg_CE6rlTjP"
      },
      "outputs": [],
      "source": [
        "data.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8X6iJdVlTjQ"
      },
      "outputs": [],
      "source": [
        "# Dropping columns which are not required\n",
        "\n",
        "data = data.drop(['dayfri', 'daymon', 'daysat', 'daysun', 'daythu','daytue', 'daywed', 'monthapr', 'monthaug', 'monthdec', \n",
        "                  'monthfeb','monthjan', 'monthjul', 'monthjun', 'monthmar', 'monthmay', 'monthnov','monthoct','monthsep'], \n",
        "                 axis = 1)\n",
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "crSbah7flTjQ"
      },
      "outputs": [],
      "source": [
        "# Checking how much datapoints are having small and large area\n",
        "data.size_category.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3PMAhvBlTjR"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "sns.countplot(x = 'size_category', data = data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "CGXLkKPYlTjS"
      },
      "outputs": [],
      "source": [
        "# Checking for which value of area is categorised into large and small by creating crosstab between area and size_category\n",
        "pd.crosstab(data.area, data.size_category)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "et2SynqYlTjS"
      },
      "outputs": [],
      "source": [
        "# Plotting Month Vs. temp plot\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams['figure.figsize'] = [20, 10]\n",
        "sns.set(style = \"darkgrid\", font_scale = 1.3)\n",
        "month_temp = sns.barplot(x = 'month', y = 'temp', data = data,\n",
        "                         order = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec'], palette = 'winter');\n",
        "month_temp.set(title = \"Month Vs Temp Barplot\", xlabel = \"Months\", ylabel = \"Temperature\");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QiWgujnolTjT"
      },
      "outputs": [],
      "source": [
        "plt.rcParams['figure.figsize'] = [10, 10]\n",
        "sns.set(style = 'whitegrid', font_scale = 1.3)\n",
        "day = sns.countplot(data['day'], order = ['sun' ,'mon', 'tue', 'wed', 'thu', 'fri', 'sat'], palette = 'spring')\n",
        "day.set(title = 'Countplot for the weekdays', xlabel = 'Days', ylabel = 'Count');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GVjqAWQFlTjT"
      },
      "outputs": [],
      "source": [
        "sns.heatmap(data.corr(), annot=True, cmap=\"inferno\")\n",
        "ax = plt.gca()\n",
        "ax.set_title(\"HeatMap of Features for the Classes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UNcIPzqlTjU"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Li1s9jl_lTjU"
      },
      "outputs": [],
      "source": [
        "# Encoding month and day features\n",
        "\n",
        "data.month.replace(('jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nov','dec'),\n",
        "                           (1,2,3,4,5,6,7,8,9,10,11,12), inplace=True)\n",
        "data.day.replace(('mon','tue','wed','thu','fri','sat','sun'),(1,2,3,4,5,6,7), inplace=True)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "e-wW3pW7lTjV"
      },
      "outputs": [],
      "source": [
        "# Encoding target variable 'size category'\n",
        "\n",
        "data.size_category.replace(('small', 'large'), (0, 1), inplace = True)\n",
        "data.sample(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kx1iYithlTjV"
      },
      "outputs": [],
      "source": [
        "data.corr()['size_category'].sort_values(ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mr41di3qlTjW"
      },
      "outputs": [],
      "source": [
        "# Standardizing data\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler=StandardScaler()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qyWL-kPllTjW"
      },
      "outputs": [],
      "source": [
        "scaler.fit(data.drop('size_category',axis=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r14eUuZxlTjW"
      },
      "outputs": [],
      "source": [
        "scaled_features=scaler.transform(data.drop('size_category',axis=1))\n",
        "data_head=pd.DataFrame(scaled_features,columns=data.columns[:-1])\n",
        "data_head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGj3wi0ulTjX"
      },
      "outputs": [],
      "source": [
        "# Splitting data into test data and train data\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(data_head,data['size_category'], test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "ARkwqfs2lTjX"
      },
      "outputs": [],
      "source": [
        "print('Shape of x_train: ', x_train.shape)\n",
        "print('Shape of x_test: ', x_test.shape)\n",
        "print('Shape of y_train: ', y_train.shape)\n",
        "print('Shape of y_test: ', y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgtV7q2klTjX"
      },
      "source": [
        "### Artificial Neural Network Model - Backpropagation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7sIuxAKlTjX"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZ9jwBL3lTjY"
      },
      "outputs": [],
      "source": [
        "# create model\n",
        "model = Sequential()\n",
        "model.add(Dense(12, input_dim=11, kernel_initializer='uniform', activation='relu'))\n",
        "model.add(Dense(10, kernel_initializer='uniform', activation='relu'))\n",
        "model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfU4iR46lTjY"
      },
      "outputs": [],
      "source": [
        "# Compile model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "w0qBGlVglTjY"
      },
      "outputs": [],
      "source": [
        "# Fit the model\n",
        "model.fit(x_train,y_train, epochs=100, batch_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ri8PtE64lTjZ"
      },
      "outputs": [],
      "source": [
        "# evaluate the model\n",
        "scores = model.evaluate(x_test, y_test)\n",
        "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXXVOtxelTjZ"
      },
      "source": [
        "### Artificial Neural Network Model - Tuning of Hyperparameters batch size and epochs "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsOGg0s8lTjZ"
      },
      "outputs": [],
      "source": [
        "# Importing the necessary packages\n",
        "from sklearn.model_selection import GridSearchCV, KFold\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.optimizers import adam_v2\n",
        "from keras.layers import Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UlepK86hlTjZ"
      },
      "outputs": [],
      "source": [
        "# create model\n",
        "def create_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(12, input_dim=11, kernel_initializer='uniform', activation='relu'))\n",
        "    model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n",
        "    model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
        "    \n",
        "    adam=adam_v2.Adam(learning_rate=0.01)\n",
        "    model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ef8_IRfBlTja"
      },
      "outputs": [],
      "source": [
        "# Create the model\n",
        "model = KerasClassifier(build_fn = create_model,verbose = 0)\n",
        "# Define the grid search parameters\n",
        "batch_size = [10,20,40]\n",
        "epochs = [10,50,100]\n",
        "# Make a dictionary of the grid search parameters\n",
        "param_grid = dict(batch_size = batch_size,epochs = epochs)\n",
        "# Build and fit the GridSearchCV\n",
        "grid = GridSearchCV(estimator = model,param_grid = param_grid,cv = KFold(),verbose = 10)\n",
        "grid_result = grid.fit(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "WoAazpLqlTja"
      },
      "outputs": [],
      "source": [
        "# Summarize the results\n",
        "print('Best : {}, using {}'.format(grid_result.best_score_,grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "  print('{},{} with: {}'.format(mean, stdev, param))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XkR3Il25lTja"
      },
      "source": [
        "## Artificial Neural Network Model - Tuning of All Hyperparameters "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k1SeygNtlTjb"
      },
      "outputs": [],
      "source": [
        "def create_model(learning_rate,dropout_rate,activation_function,init,neuron1,neuron2):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(neuron1,input_dim = 11,kernel_initializer = init,activation = activation_function))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(neuron2,input_dim = neuron1,kernel_initializer = init,activation = activation_function))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(1,activation = 'sigmoid'))\n",
        "    \n",
        "    adam=adam_v2.Adam(learning_rate = learning_rate)\n",
        "    model.compile(loss = 'binary_crossentropy',optimizer = adam,metrics = ['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Create the model\n",
        "\n",
        "model = KerasClassifier(build_fn = create_model,verbose = 0)\n",
        "\n",
        "# Define the grid search parameters\n",
        "\n",
        "batch_size = [10,20,40]\n",
        "epochs = [10,50,100]\n",
        "learning_rate = [0.001,0.01,0.1]\n",
        "dropout_rate = [0.0,0.1,0.2]\n",
        "activation_function = ['softmax','relu','tanh','linear']\n",
        "init = ['uniform','normal','zero']\n",
        "neuron1 = [4,8,16]\n",
        "neuron2 = [2,4,8]\n",
        "\n",
        "# Make a dictionary of the grid search parameters\n",
        "\n",
        "param_grids = dict(batch_size = batch_size,epochs = epochs,learning_rate = learning_rate,dropout_rate = dropout_rate,\n",
        "                   activation_function = activation_function,init = init,neuron1 = neuron1,neuron2 = neuron2)\n",
        "\n",
        "# Build and fit the GridSearchCV\n",
        "\n",
        "grid = GridSearchCV(estimator = model,param_grid = param_grids,cv = KFold(),verbose = 10)\n",
        "grid_result = grid.fit(x_train, y_train)\n",
        "\n",
        "# Summarize the results\n",
        "print('Best : {}, using {}'.format(grid_result.best_score_,grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "  print('{},{} with: {}'.format(mean, stdev, param))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybrkfdsHlTjc"
      },
      "source": [
        "#### Best : 0.9916666626930237, using {'activation_function': 'tanh', 'batch_size': 20, 'dropout_rate': 0.1, 'epochs': 50, 'init': 'normal', 'learning_rate': 0.01, 'neuron1': 4, 'neuron2': 2}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sW9pdsP1lTjd"
      },
      "source": [
        "### Building final model with above best parameters obtained from tuning hyper paramters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqru2vZnlTjd"
      },
      "outputs": [],
      "source": [
        "# create model\n",
        "final_model = Sequential()\n",
        "final_model.add(Dense(4,input_dim = 11,kernel_initializer = 'normal',activation = 'tanh'))\n",
        "final_model.add(Dropout(0.1))\n",
        "final_model.add(Dense(2,input_dim = 4,kernel_initializer = 'normal',activation = 'tanh'))\n",
        "final_model.add(Dropout(0.1))\n",
        "final_model.add(Dense(1,activation = 'sigmoid'))\n",
        "    \n",
        "adam=adam_v2.Adam(learning_rate = 0.01)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3bf2nFhKlTje"
      },
      "outputs": [],
      "source": [
        "# Compile Model\n",
        "final_model.compile(loss = 'binary_crossentropy',optimizer = adam,metrics = ['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "H8zI8S9PlTje"
      },
      "outputs": [],
      "source": [
        "# Fit the model\n",
        "final_model.fit(x_train,y_train, epochs=50, batch_size=20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uicdkFTPlTjf"
      },
      "outputs": [],
      "source": [
        "# evaluate the model\n",
        "scores = final_model.evaluate(x_test, y_test)\n",
        "print(\"%s: %.2f%%\" % (final_model.metrics_names[1], scores[1]*100))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": " Neural _Network_assng 1.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}